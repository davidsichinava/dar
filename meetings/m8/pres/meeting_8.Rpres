<div class="header" style="margin-top:0 px;font-size:60%;">Rმა: მერვე შეხვედრა</div>

რაოდენობრივ მონაცემთა ანალიზი R-გარემოში
========================================================
author: დავით სიჭინავა
date: 11 მაისი, 2018
autosize: true
transition: none
css: css/style.css
font-family: 'BPG_upper'
<span style="font-weight:bold; font-family:BPG_upper;">მერვე შეხვედრა</span>



დღევანდელი შეხვედრის გეგმა
========================================================

- წრფივი რეგრესია

ამოცანა:
========================================================
* Todorov et. al. (2005)
“Inferences of competence from faces predict election outcomes.” Science, vol. 308, no. 10 (June), pp. 1623–1626.

* როგორ შეიძლება სახის გამომეტყველებით არჩევნების შედეგების პროგნოზი



ცვლადები:
========================================================
| ცვლადი   | აღწერა                              |
|----------|-------------------------------------|
| congress | კონგრესის მოწვევა                   |
| year     | არჩევნების წელი                     |
| state    | შტატი                               |
| winner   | გამარჯვებული                        |
| loser    | დამარცხებული                        |
| w.party  | გამარჯვებული კანდიდატის პარტია      |
| l.party  | დამარცხებული კანდიდატის პარტია      |
| d.votes  | დემოკრატი კანდიდატის ხმები          |
| r.votes  | რესპუბლიკელი კანდიდატის ხმები       |
| d.comp   | დემოკრატი კანდიდატის კომპეტენცია    |
| r.comp   | რესპუბლიკელი კანდიდატის კომპეტენცია |

მონაცემები:
========================================================
```{r, eval=FALSE}
## მონაცემთა წაკითხვა. არ დაგავიწყდეთ სამუშაო ფოლდერის მითითება და ფაილის ამ ფოლდერში ჩაგდება
face <- read.csv("face.csv")
## ხმების წილი და სხვაობა
face$d.share <- face$d.votes / (face$d.votes + face$r.votes)
face$r.share <- face$r.votes / (face$d.votes + face$r.votes)
face$diff.share <- face$d.share - face$r.share

```
კორელაცია:
========================================================
* როგორია კომპეტენციასა და ხმების სხვაობას შორის ურთიერთდამოკიდებულება?

```{r, eval=FALSE}
cor(face$d.comp, face$diff.share)
```

წრფივი დამოკიდებულება:
========================================================

$Y = \alpha + \beta X + \epsilon $, 
სადაც $\alpha$ წარმოადგენს დამოუკიდებელ წევრს (ე.წ. intercept), $X$ - ამხსნელ ცვლადს, $\beta$ - რეგრესიის კოეფიციენტს, $\epsilon$ - ცდომილებას (error term).


წრფივი დამოკიდებულება:
========================================================

* ყველა მოდელი არასწორია, თუმცა ზოგიერთი სასარგებლოა;
* ჩვენი მიზანია, მეტნაკლები სიზუსტით შევაფასოთ _მონაცემთა წარმოშობის პროცესი_
* შესაბამისად, ყველა მოდელში წარმოდგენილია კოეფიციენტების _შეფასება_

წრფივი დამოკიდებულება:
========================================================
$\hat{Y} = \hat{\alpha}+\hat{\beta} x$, სადაც $x$ წარმოადგენს $X$ რომელიმე მნიშვნელობას;

წრფივი დამოკიდებულება:
========================================================
$\hat{\epsilon} = Y-\hat{Y}$, სადაც $x$ წარმოადგენს $X$ რომელიმე მნიშვნელობას;

წრფივი დამოკიდებულება:
========================================================
```{r, eval=FALSE}
fit <- lm(diff.share ~ d.comp, data = face) # fit the model
fit
```

დიაგრამა!
========================================================
```{r, eval=FALSE}
plot(face$d.comp, face$diff.share, xlim = c(0, 1.05), ylim = c(-1,1),
xlab = "Competence scores for Democrats",
ylab = "Democratic margin in vote share",
main = "Facial competence and vote share")
abline(fit) # add regression line
abline(v = 0, lty = "dashed")

```

უმცირეს კვადრატთა მეთოდი
========================================================
* საუკეთესო თეორიული წრფის შერჩევა
* ვარჩევთ იმ პარამეტრებს, რომელთა ცდომილებათა ჯამი ყველაზე მცირეა
* ცდომილებათა საშუალო კვადრატული (RMSE)
$RMSE = \sqrt{\frac{1}{n}SSR}$

უმცირეს კვადრატთა მეთოდი
========================================================
```{r, eval=FALSE}
epsilon.hat <- resid(fit) # ცდომილებები
sqrt(mean(epsilon.hat^2)) # RMSE
```

ბეტა-კოეფიციენტი:
========================================================
$\beta = corr(X, Y) * \frac{SD(Y)}{SD(X)}$

დაშვება:
========================================================
* წრფივი ურთიერთდამოკიდებულება
* ჰომოსკედასტურობა
* ცდომილებათა დამოუკიდებლობა
* ნორმალური განაწილება


რას ვამოწმებთ:
========================================================
* უკიდურეს წერტილებს
* ,,გავლენიან'' ჩანაწერებს

უკიდურესი წერტილები
========================================================
```{r, eval=FALSE}
library(car)
### outlierTest გვიჩვენებს, თუ მერამდენე ჩანაწერია ყველაზე ,,ექსტემალური'', რომელიც დიდ გავლენას ახდენს მოდელის ხარისხზე
outlierTest(fit)


```

გავლენიანი ჩანაწერები
========================================================
```{r, eval=FALSE}
### leveragePlots გვიჩვენებს, თუ რომელი ჩანაწერები ,,ექაჩებიან''  მოდელს, რაც იმაზე მიგვითითებს, რომ რეგრესიის წრფე ამ შემთხვევების გავლენით, ოპტიმალურისგან შორს მდებარეობს.
leveragePlots(fit)

# კუკის მანძილის დიაგრამები
# D მნიშვნელობის განსაზღვრა > 4/(n-k-1) 
cutoff <- 4/((nrow(face)-length(fit$coefficients)-2))

plot(fit, which=4, cook.levels=cutoff)
# ამ შემთხვევაში, ჩვენი ამოცანაა, პოტენციური პრობლემის აღმოსაფხვრელად შევისწავლოთ ყველა ის ჩანაწერი, რომელიც cutoff მნიშვნელობას აღემატება.

```

ცდომილებათა ნორმალურობა
========================================================
* MASS ბიბლიოთეკის გამოყენებით
```{r, eval=FALSE}

# გარდა ზემოთ აღწერილი მეთოდისა, ცდომილებათა ნორმალურობის შესწავლა შესაძლებელია, თუ ნაწინასწარმეტყველებ ცდომილებებს ,,სტიუდენტიზაციას'' გავუკეთებთ, ანუ - მოვახდენთ მის სტანდარტიზაციას სტანდარტული გადახრის მეშვეობით. 

library(MASS)
sresid <- studres(fit) 
hist(sresid, freq=FALSE, 
   main="სტუდენტიზებული ცდომილებების განაწილება")

```

არამუდმივ ცდომილებათა დისპერსია (ჰეტეროსკედასტურობა)
========================================================
```{r, eval=FALSE}
### ამ შემთხვევაში, უნდა გამოვიყენოთ lmtest ბიბლიოთეკა და ბრეუშ-პაგანის ტესტი
bptest(fit)

### ბრეუშ-პაგანის ტესტის ნულოვანი ჰიპოთეზა არის ის, რომ ცდომილებათად დისპერსია არ არის დამოკიდებული გარე ფაქტორებზე. შესაბამისად, თუ p-მნიშვნელობა მცირეა, ნულოვანი ჰიპოთზა უნდა უარვყოთ და ჩავთვალოთ, რომ ჩვენს მონაცემებს ჰომოსკედასტურობის პრობლემა აქვს.

### თუ მონაცემები ჰეტეროსკედასტურია, შესაბამისად, გამოთვლილი სტანდარტული შეცდომა არასწორია. პრობლემის აღმოსაფხვრელად როგორც წესი, ე.წ. რობასტულ (robust) რეგრესიულ მოდელებს იყენებენ

```

მულტიკოლინეარულობა
========================================================

* მრავალწევრი რეგრესიული მოდელის ერთ-ერთი დაშვება არის ის, რომ დამოუკიდებელ ცვლადებს შორის წრფივი დამოკიდებულება არ უნდა იყოს, ან - მცირე დონეზე. თუმცა ეს პირობა ცხოვრებაში არასდროს სრულდება;

* მულტიკოლინეარულობის აღმოჩენის ყველაზე მარტივი ხერხი ამხსნელ ცვლადებს შორის კორელაციის გამოთვლაა. თუმცა ასევე შესაძლებელია ე.წ. დისპერსიის ინფლაციის მაჩვენებლის გამოთვლაა

```{r, eval=FALSE}

vif(fit) # დისპერსიის ინფლაციის ფაქტორი 
sqrt(vif(fit)) > 2 # პრობლემატური შემთხვევები

```

მულტიკოლინეარულობა
========================================================
* როგორ მოვაგვაროთ პრობლემა? საჭიროა, გადავხედოთ მოდელის ამხსნელ ცვლადებს და დავრწმუნდეთ, ერთმანეთთან კორელაციაში მყოფი ცვლადები ერთსა და იმავე ფენომენს ზომავენ. თუ თეორიული თვალსაზრისით ასეა, მაშინ შეგვიძლია, ერთ-ერთი რომელიმე ცვლადი რეგრესიული მოდელიდან ამოვიღოთ 


